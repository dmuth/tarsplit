#!/usr/bin/env python3
#
# https://github.com/dmuth/tarsplit
#
# This script will take a tarball and split it into 2 or more parts.
# Most importantly, these parts WILL BE ALONG FILE BOUNDARIES.
# The reason for splitting along file foundaries is to that extraction
# can be done with plain old tar.
#
# The advantage of this approach is that things like larger Docker images can
# now be broken down into smaller layers, which each layer extracting a subset
# of the original tarball's directory structure.
#
#


import argparse
import os
import tarfile
from threading import Thread
import tempfile


parser = argparse.ArgumentParser(description = "Split up a tarball into 2 more chunks of roughly equal size.")
parser.add_argument("file", type = str,
	help = "The tarball to split up")
parser.add_argument("num", type = int, 
	help = "How many chunks to split into?")
parser.add_argument("--dry-run", action = "store_true",
	help = "Perform a dry run and tell what we WOULD do.")

args = parser.parse_args()
#print(args) # Debugging

#
# Check our arguments.
#
def check_args(args):
	if args.num < 2:
		raise ValueError("Number of chunks cannot be less than 2!")


#
# Calculate the size per chunk.
# This is based on uncompressed values, the resulting tarballs may vary in size.
#
def get_chunk_size(t, num):

	total_file_size = 0
	for f in t.getmembers():
		total_file_size += f.size

	retval = (total_file_size, int(total_file_size / args.num))

	return(retval)


#
# Get our filename for a specific part.
#
def open_chunkfile(file, part, num, dry_run = False):

	out = None

	num_len = len(str(num))
	part_formatted = str(part).zfill(num_len)

	filename = f"{os.path.basename(file)}-part-{part_formatted}-of-{num}"
	if not dry_run:
		out = tarfile.open(filename, "w:gz")

	return(filename, out)


#
# Get the filename of the current chunkfile
#
def get_chunkfile_name(file, part, num):

	num_len = len(str(num))
	part_formatted = str(part).zfill(num_len)
	filename = f"{os.path.basename(file)}-part-{part_formatted}-of-{num}"
	return(filename)


#
# Print out our write status for the current chunk
#
def print_write_status(filename, num_files_in_current_chunk, thread_id = ""):

	write_status = False
	if num_files_in_current_chunk < 100:
		if num_files_in_current_chunk % 10 == 0:
			write_status = True
	elif num_files_in_current_chunk < 1000:
		if num_files_in_current_chunk % 100 == 0:
			write_status = True
	else:
		if num_files_in_current_chunk % 1000 == 0:
			write_status = True

	if write_status:
		if thread_id:
			print(f"{thread_id}: {num_files_in_current_chunk} files written to {filename}")
		else:
			print(f"{num_files_in_current_chunk} files written to {filename}")


#
# Our main entrypoint.
#
def main(args):

	check_args(args)

	t = tarfile.open(args.file, "r")

	print(f"Welcome to Tarsplit! Reading file {args.file}...")
	(total_file_size, chunk_size) = get_chunk_size(t, args.num)

	print(f"Total uncompressed file size: {total_file_size} bytes, "
		+ f"num chunks: {args.num}, chunk size: {chunk_size} bytes")

	(filename, out) = open_chunkfile(args.file, 1, args.num, dry_run = args.dry_run)

	size = 0
	current_chunk = 1
	num_files_in_current_chunk = 0

	#
	# Loop through are files, and write them out to separate tarballs.
	#
	for f in t.getmembers():

		name = f.name
		size += f.size

		f = t.extractfile(name)
		info = t.getmember(name)
		if not args.dry_run:
			out.addfile(info, f)

		num_files_in_current_chunk += 1
		print_write_status(filename, num_files_in_current_chunk)

		if current_chunk < args.num:
			if size >= chunk_size:

				if not args.dry_run:
					out.close()
					print(f"Successfully wrote {size} bytes in {num_files_in_current_chunk} files to {filename}")
				else:
					print(f"Would have written {size} bytes in {num_files_in_current_chunk} files to {filename}")

				size = 0
				current_chunk += 1
				num_files_in_current_chunk = 0

				(filename, out) = open_chunkfile(args.file, current_chunk, args.num, 
					dry_run = args.dry_run)

	t.close()

	if not args.dry_run:
		print(f"Successfully wrote {size} bytes in {num_files_in_current_chunk} files to {filename}")
		out.close()
	else:
		print(f"Would have written {size} bytes in {num_files_in_current_chunk} files to {filename}")


#
# Our main entrypoint for a thread.
# It takes a thread ID, filename to write to, and a list of files to write to that filename.
#
def threadMain(thread_id, filename, sourcedir, chunk_filename, chunk_files, dry_run, tempdir):

	t = tarfile.open(filename, "r")

	size = 0
	out = None
	num_files_written = 0
	outfile = f"{sourcedir}/{chunk_filename}"
	if not dry_run:
		out = tarfile.open(outfile, "w:gz")

	print(f"{thread_id}: Going through chunk files...")
	for file in chunk_files:
		#f = t.extractfile(file["name"])
		info = file["info"]
		#print(file["name"])
		if not args.dry_run:
			#out.addfile(info, f)
			out.add(file["name"])
		size += info.size
		num_files_written += 1
		print_write_status(chunk_filename, num_files_written, thread_id = thread_id)

	if not dry_run:
		out.close()
		print(f"{thread_id}: Successfully wrote {size} bytes in {len(chunk_files)} files to {outfile}")
	else:
		print(f"Would have written {size} bytes in {num_files_in_current_chunk} files to {outfile}")

	t.close()


#
# Go through our tarfile and break it up into chunks.
#
def get_chunks(file, num_chunks):

	files = []
	chunks = []
	total_file_size = 0

	#
	# Go through the tarfile and grab all of our files and sizes.
	#
	t = tarfile.open(file, "r")
	for f in t.getmembers():
		total_file_size += f.size
		name = f.name
		size = f.size
		info = t.getmember(name)
		files.append({"name": name, "size": size, "info": info})
		#files.append(name)

	t.close()
	chunk_size = int(total_file_size / num_chunks)

	#
	# Now go through our files and break them into chunks.
	#
	chunk = []
	current_chunk = 1
	size = 0
	for file in files:
		# TEST I should pull out info
		chunk.append({"name": file["name"], "info": file["info"]})
		size += file["size"]

		#
		# Only check sizes if we're not on the final chunk.
		#
		if current_chunk < num_chunks:
			if size >= chunk_size:

				current_chunk += 1
				size = 0

				chunks.append(chunk)
				chunk = []


	# Append the final chunk
	chunks.append(chunk)

	retval = (total_file_size, chunk_size, chunks)

	return(retval)


#
# Put the files in the tarball into an list of chunks where each chunk is a list of files,
# and then send each chunk to a separate thread.
#
def mainThreaded(args):

	check_args(args)
	source = os.path.abspath(args.file)
	sourcedir = os.getcwd()
	tmp = tempfile.TemporaryDirectory(prefix = "tarsplit-")
	print(f"Extracting tarball to temp directory: {tmp.name}")

	t = tarfile.open(source, "r")
	t.extractall(path = tmp.name, numeric_owner = True)
	os.chdir(tmp.name)
	#print(tmp.name); 
	#import time; time.sleep(60)

	print(f"Welcome to Tarsplit! Reading file {args.file}...")
	(total_file_size, chunk_size, chunks) = get_chunks(source, args.num)

	print(f"Total uncompressed file size: {total_file_size} bytes, "
		+ f"num chunks: {args.num}, chunk size: {chunk_size} bytes")

	threads = []
	i = 0
	for chunk in chunks:
		chunk_name = get_chunkfile_name(args.file, (i + 1), args.num)
		thread_id = f"Thread ID #{i}"
		thread = Thread(target = threadMain, name = thread_id, 
			args = (thread_id, source, sourcedir, chunk_name, chunk, args.dry_run, tmp.name))
		threads.append(thread)
		i += 1

	for thread in threads:
		thread.start()

	for thread in threads:
		print(f"Waiting for thread {thread.name} to finish...")
		thread.join()

	tmp.cleanup()


#main(args)
mainThreaded(args)


